{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Update Parameters Here\n",
    "\"\"\"\n",
    "COLLECTION_NAME = \"Quaks\"\n",
    "CONTRACT = \"0x07bbdaf30e89ea3ecf6cadc80d6e7c4b0843c729\"\n",
    "CHAIN = \"eth\"\n",
    "\n",
    "\"\"\" \n",
    "Optional parameters\n",
    "\"\"\"\n",
    "FOLDER = 1\n",
    "\n",
    "KEEP_ALL_DATA = False  # set to TRUE to keep the raw JSON on disk\n",
    "\n",
    "MAX_RESULTS = 500  # max results per request\n",
    "TIME_DELTA = 0.1  # time to wait between successful calls\n",
    "TIME_DELTA_2 = 5  # time to wait after API throttling message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rarity data loaded from: /home/erwin/dev-workspace/crypto/nft-shenanigans/honestnft-shenanigans/data/rarity_data/Quaks_raritytools.csv\n",
      "Getting minting data for Quaks\n",
      "getting page 1 ...\n",
      "Sucessfuly received page 1 of 18\n",
      "getting page 2 ...\n",
      "Sucessfuly received page 2 of 18\n",
      "getting page 3 ...\n",
      "Sucessfuly received page 3 of 18\n",
      "getting page 4 ...\n",
      "Sucessfuly received page 4 of 18\n",
      "getting page 5 ...\n",
      "Sucessfuly received page 5 of 18\n",
      "getting page 6 ...\n",
      "Sucessfuly received page 6 of 18\n",
      "getting page 7 ...\n",
      "Sucessfuly received page 7 of 18\n",
      "getting page 8 ...\n",
      "Sucessfuly received page 8 of 18\n",
      "getting page 9 ...\n",
      "Sucessfuly received page 9 of 18\n",
      "getting page 10 ...\n",
      "Sucessfuly received page 10 of 18\n",
      "getting page 11 ...\n",
      "Sucessfuly received page 11 of 18\n",
      "getting page 12 ...\n",
      "Sucessfuly received page 12 of 18\n",
      "getting page 13 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 13 ...\n",
      "Sucessfuly received page 13 of 18\n",
      "getting page 14 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 14 ...\n",
      "Sucessfuly received page 14 of 18\n",
      "getting page 15 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 15 ...\n",
      "Sucessfuly received page 15 of 18\n",
      "getting page 16 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 16 ...\n",
      "Sucessfuly received page 16 of 18\n",
      "getting page 17 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 17 ...\n",
      "Sucessfuly received page 17 of 18\n",
      "getting page 18 ...\n",
      "Got a 429 response from the server. Waiting 5 seconds and retrying\n",
      "getting page 18 ...\n",
      "Sucessfuly received page 18 of 18\n",
      "--- 42.3 seconds ---\n",
      "finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1059714/1972671154.py:112: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"time\"] = df[\"time\"].str.replace(\".000Z\", \"\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "A Moralis API Key is required.\n",
    "The free tier includes one and is enough to get minting data.\n",
    "\n",
    "AVAILABLE CHAINS:\n",
    "    eth, ropsten, rinkeby, goerli, kovan,\n",
    "    polygon, mumbai, bsc, bsc testnet,\n",
    "    avalanche, avalanche testnet, fantom\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "\n",
    "from honestnft_utils import config\n",
    "\n",
    "\n",
    "def get_mintdata(\n",
    "    COLLECTION_NAME,\n",
    "    CONTRACT,\n",
    "    CHAIN,\n",
    "):\n",
    "\n",
    "    RARITY_CSV = f\"{config.RARITY_FOLDER}/{COLLECTION_NAME}_raritytools.csv\"\n",
    "\n",
    "    print(f\"Rarity data loaded from: {RARITY_CSV}\")\n",
    "    RARITY_DB = pd.read_csv(RARITY_CSV)\n",
    "\n",
    "    headers = {\"Content-type\": \"application/json\", \"x-api-key\": config.MORALIS_API_KEY}\n",
    "\n",
    "    print(f\"Getting minting data for {COLLECTION_NAME}\")\n",
    "    more_results = True\n",
    "    page = 0\n",
    "    start_time = time.time()\n",
    "    all_data = list()  # empty list to store data as it comes\n",
    "    while more_results:\n",
    "        url = \"https://deep-index.moralis.io/api/v2/nft/{}/transfers?chain={}&format=decimal&offset={}&limit={}\".format(\n",
    "            CONTRACT, CHAIN, page * 500, MAX_RESULTS\n",
    "        )\n",
    "        print(f\"getting page {page + 1} ...\")\n",
    "\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\n",
    "                \"Sucessfuly received page {} of {}\".format(\n",
    "                    page + 1, int(1 + response.json()[\"total\"] / MAX_RESULTS)\n",
    "                )\n",
    "            )\n",
    "            PATH = (\n",
    "                f\"{config.MINTING_FOLDER}/{COLLECTION_NAME}/{page * MAX_RESULTS}.json\"\n",
    "            )\n",
    "\n",
    "            # add new data to existing list\n",
    "            all_data.extend(response.json()[\"result\"])\n",
    "\n",
    "            page += 1\n",
    "\n",
    "            # if results in this response is less than MAX_RESULTS then it's the last page\n",
    "            if len(response.json()[\"result\"]) < MAX_RESULTS:\n",
    "                more_results = False\n",
    "            else:\n",
    "                time.sleep(TIME_DELTA)\n",
    "\n",
    "        elif response.status_code in [429, 503, 520]:\n",
    "            print(\n",
    "                f\"Got a {response.status_code} response from the server. Waiting {TIME_DELTA_2} seconds and retrying\"\n",
    "            )\n",
    "            time.sleep(TIME_DELTA_2)\n",
    "\n",
    "        else:\n",
    "            print(f\"status_code = {response.status_code}\")\n",
    "            print(\"Received a unexpected error from Moralis API. Closing process.\")\n",
    "            more_results = False\n",
    "\n",
    "    # Save full json data to one master file\n",
    "    if KEEP_ALL_DATA:\n",
    "        folder = f\"{config.MINTING_FOLDER}/{COLLECTION_NAME}/\"\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "        PATH = f\"{config.MINTING_FOLDER}/{COLLECTION_NAME}/{COLLECTION_NAME}.json\"\n",
    "        with open(PATH, \"w\") as destination_file:\n",
    "            json.dump(all_data, destination_file)\n",
    "\n",
    "    df = json_normalize(all_data)\n",
    "    # remove non minting rows\n",
    "    df = df.loc[df[\"from_address\"] == \"0x0000000000000000000000000000000000000000\"]\n",
    "\n",
    "    # make sure token_id is an integer\n",
    "    df[\"token_id\"] = df[\"token_id\"].astype(int)\n",
    "\n",
    "    # add rarity rank to minting data\n",
    "    df = df.merge(RARITY_DB, left_on=\"token_id\", right_on=\"TOKEN_ID\")\n",
    "\n",
    "    # discard unwanted columns\n",
    "    df = df[\n",
    "        [\n",
    "            \"transaction_hash\",\n",
    "            \"to_address\",\n",
    "            \"token_id\",\n",
    "            \"from_address\",\n",
    "            \"Rank\",\n",
    "            \"block_timestamp\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    df.drop_duplicates(subset=[\"token_id\"], inplace=True)\n",
    "\n",
    "    # get matching columns names to HonestNFT csv format\n",
    "    df.columns = [\"txid\", \"to_account\", \"TOKEN_ID\", \"current_owner\", \"rank\", \"time\"]\n",
    "\n",
    "    # clean 'time' field to make it compatible with the csv produced by 'find_minting_data.ipynb'\n",
    "    df[\"time\"] = df[\"time\"].str.replace(\".000Z\", \"\")\n",
    "\n",
    "    df.to_csv(f\"{config.MINTING_FOLDER}/{COLLECTION_NAME}_minting.csv\")\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (round(time.time() - start_time, 1)))\n",
    "    print(\"finished\")\n",
    "\n",
    "\n",
    "get_mintdata(COLLECTION_NAME, CONTRACT, CHAIN)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
